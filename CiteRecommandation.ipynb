{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Système de Recommandation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPfyUVTRox-V"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "googleAuth = GoogleAuth()\n",
        "googleAuth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(googleAuth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJBDShogpA1M"
      },
      "source": [
        "file_path = 'https://drive.google.com/file/d/1pBVuyN2jSITNfJDvO9cgIxZIwLdkSTft/view?usp=sharing'\n",
        "id = '1pBVuyN2jSITNfJDvO9cgIxZIwLdkSTft'\n",
        "CSVfile = drive.CreateFile({'id':id}) \n",
        "CSVfile.GetContentFile('BDD-Citation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0KLu-ey75vI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import string\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import wordnet\n",
        "from pprint import pprint\n",
        "from gensim import corpora, models\n",
        "from gensim.utils import simple_preprocess, lemmatize\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from textblob import TextBlob, Word \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def Process_Data(row_index, column_index):\n",
        "\n",
        "  Context = str(reader.iloc[row_index][column_index])\n",
        "  Context_without_ref = re.sub(r'{.+?}', ' ', Context)\n",
        "  Context_without_number = re.sub(r'\\d+', ' ', Context_without_ref.lower())\n",
        "  Context_without_punct = Context_without_number.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
        "  Context_without_multiple_spaces = re.sub(' +', ' ', Context_without_punct)\n",
        "\n",
        "  return Context_without_multiple_spaces\n",
        "\n",
        "\n",
        "def TextBlob_Lemmatize(text):\n",
        "    word = TextBlob(text)\n",
        "    tag_dict = {\"J\": 'a', \"N\": 'n', \n",
        "                \"V\": 'v', \"R\": 'r'}\n",
        "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in word.tags]    \n",
        "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "    return lemmatized_list[0]\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(stemmer.stem(TextBlob_Lemmatize(token)))\n",
        "    return result\n",
        "\n",
        "columns = ['ReferenceID', 'SourceID', 'ChapterNumber', 'ParagraphNumber',\n",
        "           'SentenceNumber', 'Title', 'PublishedDate', 'Authors',\n",
        "           'TextBeforeRefMention', 'TextWhereRefMention', 'TextAfterRefMention']\n",
        "\n",
        "reader = pd.read_csv('BDD-Citation.csv', sep = ',', names = columns, encoding = 'ISO-8859-1', nrows = 9000)\n",
        "Right_Contexts = pd.DataFrame(columns = ['Right_Contexts'])\n",
        "Left_Contexts = pd.DataFrame(columns = ['Left_Contexts'])\n",
        "Scientific_Paper = pd.DataFrame(columns = ['Scientific_Paper'])\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "for i in range(reader.shape[0]):\n",
        "  if str(reader.iloc[i][8]) != \"\":\n",
        "    Left_Contexts.loc[reader.index[i]] = Process_Data(i, 8)\n",
        "  if str(reader.iloc[i][10]) != \"\":\n",
        "    Right_Contexts.loc[reader.index[i]] = Process_Data(i, 10)\n",
        "  Scientific_Paper.loc[reader.index[i]] = Process_Data(i, 5) + \" \" + Process_Data(i, 8) + \" \" + Process_Data(i, 9) + \" \" + Process_Data(i, 10)\n",
        "\n",
        "Left_Lemma = Left_Contexts['Left_Contexts'].map(preprocess)\n",
        "Right_Lemma = Right_Contexts['Right_Contexts'].map(preprocess)\n",
        "Paper_Lemma = Scientific_Paper['Scientific_Paper'].map(preprocess)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6M8-Obm9dUA"
      },
      "source": [
        "!pip install pyLDAvis\n",
        "from tqdm._tqdm_notebook import tnrange,tqdm\n",
        "from gensim.models import CoherenceModel\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import pyLDAvis.gensim\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(Paper_Lemma)\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in Paper_Lemma]\n",
        "\n",
        "Lda = models.LdaMulticore\n",
        "coherenceList_umass = []\n",
        "coherenceList_cv = []\n",
        "\n",
        "num_topics_list = np.arange(30, 50, 2)\n",
        "for num_topics in tqdm(num_topics_list):\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    lda = Lda(bow_corpus, num_topics = num_topics, id2word = dictionary, minimum_probability = 0, passes = 20)\n",
        "    cm = CoherenceModel(model = lda, corpus = bow_corpus, dictionary = dictionary, coherence = 'u_mass')\n",
        "    coherenceList_umass.append(cm.get_coherence())\n",
        "    cm_cv = CoherenceModel(model = lda, corpus = bow_corpus, texts = Paper_Lemma, dictionary = dictionary, coherence = 'c_v')\n",
        "    coherenceList_cv.append(cm_cv.get_coherence())\n",
        "    vis = pyLDAvis.gensim.prepare(lda, bow_corpus, dictionary)\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "\n",
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = num_topics_list[max_cv], alpha = 0.1, eta = 0.01, random_state = 123, id2word = dictionary, passes = 2, workers = 2)\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "  print('Topic: {} Word: {}'.format(idx, topic))\n",
        "\n",
        "seuil, Candidate_Articles, First_Topic = 0, [], []\n",
        "for i in range(0, reader.shape[0]):\n",
        "  for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key = lambda tup: -1*tup[1]):\n",
        "    if seuil == 0: \n",
        "      Topic_Index = index\n",
        "      First_Topic.append(index)\n",
        "      break\n",
        "    else:\n",
        "      seuil += 1\n",
        "\n",
        "\n",
        "test = input(\"Entrez un contexte : \")\n",
        "print(First_Topic[int(test)])\n",
        "\n",
        "for i in range(0, reader.shape[0]):\n",
        "  Position = 0\n",
        "  for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key = lambda tup: -1*tup[1]):\n",
        "    if Position == 3:\n",
        "      break\n",
        "    else:\n",
        "      if index == First_Topic[int(test)]:\n",
        "        Candidate_Articles.append(i)\n",
        "      Position += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_XpavB6SiRD"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "def Word2Vec_Model(Lemma):\n",
        "  model = Word2Vec(Lemma, size = 100, window = 5, sg = 0, min_count = 1)\n",
        "  model.train(Lemma, total_examples = model.corpus_count, epochs = 100)\n",
        "  sentence_list = []\n",
        "  for sentence in Lemma:\n",
        "    if not sentence: \n",
        "      sentence_list.append(\"null\")\n",
        "    else:\n",
        "      word_list = []\n",
        "      for word in sentence:\n",
        "        word_list.append(model.wv[word])\n",
        "      sentence_list.append(word_list)\n",
        "  return sentence_list\n",
        "\n",
        "Left_Sentences_List = Word2Vec_Model(Left_Lemma)\n",
        "Right_Sentences_List = Word2Vec_Model(Right_Lemma)\n",
        "Paper_Sentences_List = Word2Vec_Model(Paper_Lemma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwakNoXYVCc0"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def Tensors_from_Word2Vec(Sentences_List):\n",
        "  sentence_tensors_list = []\n",
        "  for i in range(0, len(Sentences_List)):\n",
        "    if Sentences_List[i] != 'null':\n",
        "      sentence_tensors_list.append(torch.FloatTensor(list(map(list, zip(*Sentences_List[i])))))\n",
        "    else:\n",
        "      sentence_tensors_list.append(\"null\")\n",
        "  return sentence_tensors_list\n",
        "\n",
        "\n",
        "Left_Tensors_List = Tensors_from_Word2Vec(Left_Sentences_List)\n",
        "Right_Tensors_List = Tensors_from_Word2Vec(Right_Sentences_List)\n",
        "Paper_Tensors_List = Tensors_from_Word2Vec(Paper_Sentences_List)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypu9x9NXVLko"
      },
      "source": [
        "FILTER_HEIGHT = 100\n",
        "\n",
        "class CNN_Model(nn.Module):\n",
        "  def __init__(self, FILTER_WIDTH):\n",
        "    super(CNN_Model, self).__init__()\n",
        "    self.conv = nn.Conv1d(1, 1, (FILTER_HEIGHT, FILTER_WIDTH), padding_mode = \"zeros\", padding = 20)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = torch.tanh(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def CNN_Sentence_Representation(Tensors_List):\n",
        "  sentences_representation_list = []\n",
        "  for i in range(0, len(Paper_Tensors_List)):\n",
        "    if Tensors_List[i] != 'null':\n",
        "      sentence_features = torch.Tensor(0)\n",
        "      for filter_width in range(2, 17):\n",
        "        cnn = CNN_Model(filter_width)\n",
        "        max_over_time_pool = torch.max(cnn(Tensors_List[i].unsqueeze(0).unsqueeze(0)))\n",
        "        sentence_features = torch.cat((sentence_features, max_over_time_pool.unsqueeze(0)))\n",
        "      sentences_representation_list.append(sentence_features.unsqueeze(0).unsqueeze(0))\n",
        "    else:\n",
        "      sentences_representation_list.append(\"null\")\n",
        "  return sentences_representation_list\n",
        "\n",
        "def CNN_Sentence_Representation1(Tensors_List,test):\n",
        "  sentences_representation_list = []\n",
        "  if Tensors_List[test] != 'null':\n",
        "    sentence_features = torch.Tensor(0)\n",
        "    for filter_width in range(2, 17):\n",
        "      cnn = CNN_Model(filter_width)\n",
        "      max_over_time_pool = torch.max(cnn(Tensors_List[i].unsqueeze(0).unsqueeze(0)))\n",
        "      sentence_features = torch.cat((sentence_features, max_over_time_pool.unsqueeze(0)))\n",
        "    sentences_representation_list.append(sentence_features.unsqueeze(0).unsqueeze(0))\n",
        "  else:\n",
        "    sentences_representation_list.append(\"null\")\n",
        "  return sentences_representation_list\n",
        "\n",
        "Left_CNN_Representation = CNN_Sentence_Representation1(Left_Tensors_List,int(test))\n",
        "Right_CNN_Representation = CNN_Sentence_Representation1(Right_Tensors_List,int(test))\n",
        "Paper_CNN_Representation = CNN_Sentence_Representation(Paper_Tensors_List)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFvMUoIZXAV3"
      },
      "source": [
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout):\n",
        "      super(LSTM_Model, self).__init__()\n",
        "      self.dropout = nn.Dropout(p = dropout)\n",
        "      self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1, bidirectional = False)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.dropout(x)\n",
        "      _outputs, (final_hidden, _final_cell) = self.lstm(x)\n",
        "      return final_hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Y1nOF1XLEO",
        "outputId": "854c5307-79c1-49ca-9422-ff47f46e45a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def LSTM_Context_Representation(CNN_Representation, lstm, criterion):\n",
        "  lstm_list = []\n",
        "        \n",
        "  for i in range(0, len(CNN_Representation)):\n",
        "          \n",
        "    if CNN_Representation[i] != 'null': \n",
        "      output = lstm(CNN_Representation[i])\n",
        "      lstm_list.append(output)\n",
        "    else:\n",
        "      lstm_list.append(\"null\")\n",
        "  print('Finished Training') \n",
        "  return lstm_list\n",
        "\n",
        "\n",
        "def LSTM_Paper_Representation(CNN_Representation, lstm):\n",
        "  lstm_list = []\n",
        "  \n",
        "  for i in range(0, len(CNN_Representation)):\n",
        "     if CNN_Representation[i] != 'null':\n",
        "        output= lstm(CNN_Representation[i])\n",
        "        lstm_list.append(output)\n",
        "\n",
        "     else:\n",
        "        lstm_list.append(\"null\")\n",
        "  return lstm_list\n",
        "\n",
        "\n",
        "LSTM_Context = LSTM_Model(15, 15, 0.1)\n",
        "Criterion_Context = nn.MSELoss()\n",
        "LSTM_Paper = LSTM_Model(15, 30, 0.1)\n",
        "\n",
        "Left_LSTM = LSTM_Context_Representation(Left_CNN_Representation, LSTM_Context, Criterion_Context)\n",
        "Right_LSTM = LSTM_Context_Representation(Right_CNN_Representation, LSTM_Context, Criterion_Context)\n",
        "Paper_LSTM = LSTM_Paper_Representation(Paper_CNN_Representation, LSTM_Paper)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBC_u4FVX6Qy"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP_Model(nn.Module):\n",
        "  def __init__(self, size):\n",
        "      super(MLP_Model, self).__init__()\n",
        "      self.size = size\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Linear(size, size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(size, size))\n",
        "      \n",
        "  def forward(self, x):\n",
        "      x = self.layers(x)\n",
        "      return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtwzFA8FX-RO"
      },
      "source": [
        "mlp = MLP_Model(30)\n",
        "MLP_Contexts_List = []\n",
        "for i in range(0, 1):\n",
        "  print(i)\n",
        "  Combined_Contexts = None\n",
        "  if Left_LSTM[i] == 'null':\n",
        "    Combined_Contexts = Right_LSTM[i]\n",
        "\n",
        "  elif Right_LSTM[i] == 'null':\n",
        "    Combined_Contexts = Left_LSTM[i]\n",
        "\n",
        "  else:\n",
        "    Combined_Contexts = torch.cat((Left_LSTM[i], Right_LSTM[i]), 1)\n",
        "\n",
        "  MLP_Contexts_List.append(mlp(Combined_Contexts))\n",
        "\n",
        "print(Candidate_Articles)\n",
        "\n",
        "Relevance_Score = []\n",
        "\n",
        "for i in range(0, reader.shape[0]):\n",
        "  Relevance_Score.append(torch.matmul(Paper_LSTM[i], torch.transpose(MLP_Contexts_List[0], 0, 1)))\n",
        "\n",
        "dictio = {}\n",
        "for i in range(0, len(Candidate_Articles)):\n",
        "  dictio[Candidate_Articles[i]] = Relevance_Score[Candidate_Articles[i]].item()\n",
        "print({k: v for k, v in sorted(dictio.items(), key = lambda item: item[1], reverse = True)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVACraCbOwyj"
      },
      "source": [
        "print(\"le nombre d article correctement recommandées est: \", len(Candidate_Articles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlOG-l1ukRqB"
      },
      "source": [
        "\n",
        "MRR = 0\n",
        "\n",
        "for i in range(1, len(Sorted_Dict) + 1):\n",
        "  if reader.iloc[Sorted_Dict[i-1][0]][1] == reader.iloc[int(test)][1]:\n",
        "    MRR = MRR + (1 / i)\n",
        "\n",
        "MRR = MRR * (1 / len(Sorted_Dict))\n",
        "print(MRR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKqyYd2D84HY"
      },
      "source": [
        "dictio1 = {}\n",
        "R=[10,20,30,50,80,100]\n",
        "for i in range(0, len( Relevance_Score)):\n",
        "  dictio1[i] = Relevance_Score[i].item()\n",
        "\n",
        "list_key = []\n",
        "Sorted_Dictio = {k: v for k, v in sorted(dictio1.items(), key = lambda item: item[1], reverse = True)}\n",
        "for key in Sorted_Dictio.keys():\n",
        "  list_key.append(key)\n",
        "\n",
        "for element in R:\n",
        "  p = 0\n",
        "  for i in range(0, element):\n",
        "    if list_key[i] in Candidate_Articles:\n",
        "      p + = 1\n",
        "  print(\"Avec un rappel =\", element, \", le résultat est\", (p / element))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}